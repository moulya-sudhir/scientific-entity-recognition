-DOCSTART- O
CERES B-MethodName
: O
Pretraining O
of O
Graph-Conditioned O
Transformer O
for O
Semi-Structured O
Session O
Data O

User O
sessions O
empower O
many O
search O
and O
recommendation O
tasks O
on O
a O
daily O
basis. O
Such O
session O
data O
are O
semi-structured O
, O
which O
encode O
heterogeneous O
relations O
between O
queries O
and O
products O
, O
and O
each O
item O
is O
described O
by O
the O
unstructured O
text. O
Despite O
recent O
advances O
in O
self-supervised O
learning O
for O
text O
or O
graphs O
, O
there O
lack O
of O
self-supervised O
learning O
models O
that O
can O
effectively O
capture O
both O
intra-item O
semantics O
and O
inter-item O
interactions O
for O
semi-structured O
sessions. O
To O
fill O
this O
gap O
, O
we O
propose O
CERES B-MethodName
, O
a O
graph-based O
transformer O
model O
for O
semi-structured O
session O
data. O
CERES B-MethodName
learns O
representations O
that O
capture O
both O
inter-and O
intra-item O
semantics O
with O
( O
1 O
) O
a O
graph-conditioned O
masked O
language O
pretraining O
task O
that O
jointly O
learns O
from O
item O
text O
and O
item-item O
relations O
; O
and O
( O
2 O
) O
a O
graph-conditioned O
transformer O
architecture O
that O
propagates O
inter-item O
contexts O
to O
itemlevel O
representations. O
We O
pretrained O
CERES B-MethodName
using O
∼468 O
million O
Amazon O
sessions O
and O
find O
that O
CERES B-MethodName
outperforms O
strong O
pretraining O
baselines O
by O
up O
to O
9 O
% O
in O
three O
session O
search O
and O
entity O
linking O
tasks O
. O

Introduction O
User O
sessions O
are O
ubiquitous O
in O
online O
e-commerce O
stores. O
An O
e-commerce O
session O
contains O
customer O
interactions O
with O
the O
platform O
in O
a O
continuous O
period. O
Within O
one O
session O
, O
the O
customer O
can O
issue O
multiple O
queries O
and O
take O
various O
actions O
on O
the O
retrieved O
products O
for O
these O
queries O
, O
such O
as O
clicking O
, O
adding O
to O
cart O
, O
and O
purchasing. O
Sessions O
are O
important O
in O
many O
e-commerce O
applications O
, O
e.g. O
, O
product O
recommendation O
( O
Wu O
et O
al. O
, O
2019a O
) O
, O
query O
recommendation O
( O
Cucerzan O
and O
White O
, O
2007 O
) O
, O
and O
query O
understanding O
( O
Zhang O
et O
al. O
, O
2020 O
) O
. O

This O
paper O
considers O
sessions O
as O
semi-structured O
data O
, O
as O
illustrated O
in O
Figure O
1. O
At O
the O
higher O
level O
, O
sessions O
are O
heterogeneous O
graphs O
that O
contain O
interactions O
between O
items. O
At O
the O
lower O
level O
, O
each O
graph O
node O
has O
unstructured O
text O
descriptions O
: O
we O
can O
describe O
queries O
by O
search O
keywords O
and O
products O
by O
titles O
, O
attributes O
, O
customer O
reviews O
, O
and O
other O
descriptors. O
Our O
goal O
is O
to O
simultaneously O
encode O
both O
the O
graph O
and O
text O
aspects O
of O
the O
session O
data O
to O
understand O
customer O
preferences O
and O
intents O
in O
a O
session O
context O
. O

Pretraining O
on O
semi-structured O
session O
data O
remains O
an O
open O
problem. O
First O
, O
existing O
works O
on O
learning O
from O
session O
data O
usually O
treat O
a O
session O
as O
a O
sequence O
or O
a O
graph O
( O
Xu O
et O
al. O
, O
2019 O
; O
You O
et O
al. O
, O
2019 O
; O
Qiu O
et O
al. O
, O
2020b O
) O
. O
While O
they O
can O
model O
inter-item O
relations O
, O
they O
do O
not O
capture O
the O
rich O
intra-item O
semantics O
when O
text O
descriptions O
are O
available. O
Furthermore O
, O
these O
models O
are O
usually O
large O
neural O
networks O
that O
require O
massive O
labeled O
data O
to O
train O
from O
scratch. O
Another O
line O
of O
research O
utilizes O
large-scale O
pretrained O
language O
models O
( O
Lan O
et O
al. O
, O
2019 O
; O
Clark O
et O
al. O
, O
2020 O
) O
as O
text O
encoders O
for O
session O
items. O
However O
, O
they O
fail O
to O
model O
the O
relational O
graph O
structure. O
Several O
works O
attempt O
to O
improve O
language O
models O
with O
a O
graph-structured O
knowledge O
base O
, O
such O
as O
in O
( O
Liu O
et O
al. O
, O
2020 O
; O
Yao O
et O
al. O
, O
2019 O
; O
. O
While O
adjusting O
the O
semantics O
of O
entities O
according O
to O
the O
knowledge O
graph O
, O
they O
fail O
to O
encode O
general O
graph O
structures O
in O
sessions O
. O

We O
propose O
CERES B-MethodName
( O
Graph B-MethodName
Conditioned I-MethodName
Encoder I-MethodName
Representations I-MethodName
for I-MethodName
Session I-MethodName
Data I-MethodName
) O
, O
a O
pretraining O
model O
for O
semi-structured O
e-commerce O
session O
data O
, O
which O
can O
serve O
as O
a O
generic O
session O
encoder O
that O
simultaneously O
captures O
both O
intra-item O
semantics O
and O
inter-item O
relations. O
Beyond O
training O
a O
potent O
language O
model O
for O
intra-item O
semantics O
, O
our O
model O
also O
conditions O
the O
language O
modeling O
task O
on O
graph-level O
session O
information O
, O
thus O
encouraging O
the O
pretrained O
model O
to O
learn O
how O
to O
utilize O
inter-item O
signals. O
Our O
model O
architecture O
tightly O
integrates O
two O
key O
components O
: O
( O
1 O
) O
an O
item O
Transformer O
encoder O
, O
which O
captures O
text O
semantics O
of O
session O
items O
; O
and O
( O
2 O
) O
a O
graph O
conditioned O
Transformer O
, O
which O
aggregates O
and O
propagates O
inter-item O
relations O
for O
cross-item O
prediction. O
As O
a O
result O
, O
CERES B-MethodName
models O
the O
higher-level O
interactions O
between O
items O
. O

We O
have O
pretrained O
CERES B-MethodName
using O
468,199,822 O
sessions O
and O
performed O
experiments O
on O
three O
session-based O
tasks O
: O
product O
search O
, O
query O
search O
, O
and O
entity O
linking. O
By O
comparing O
with O
publicly O
available O
state-of-the-art O
language O
models O
and O
domain-specific O
language O
models O
trained O
on O
alternative O
representations O
of O
session O
data O
, O
we O
show O
that O
CERES B-MethodName
outperforms O
strong O
baselines O
on O
various O
session-based O
tasks O
by O
large O
margins. O
Experiments O
show O
that O
CERES B-MethodName
can O
effectively O
utilize O
sessionlevel O
information O
for O
downstream O
tasks O
, O
better O
capture O
text O
semantics O
for O
session O
items O
, O
and O
perform O
well O
even O
with O
very O
scarce O
training O
examples O
. O

We O
summarize O
our O
contributions O
as O
follows O
: O
1 O
) O
We O
propose O
CERES B-MethodName
, O
a O
pretrained O
model O
for O
semistructured O
e-commerce O
session O
data. O
CERES B-MethodName
can O
effectively O
encode O
both O
e-commerce O
items O
and O
sessions O
and O
generically O
support O
various O
sessionbased O
downstream O
tasks. O
2 O
) O
We O
propose O
a O
new O
graph-conditioned O
transformer O
model O
for O
pretraining O
on O
general O
relational O
structures O
on O
text O
data. O
3 O
) O
We O
conducted O
extensive O
experiments O
on O
a O
largescale O
e-commerce O
benchmark O
for O
three O
sessionrelated O
tasks. O
The O
results O
show O
the O
superiority O
of O
CERES B-MethodName
over O
strong O
baselines O
, O
including O
mainstream O
pretrained O
language O
models O
and O
state-ofthe-art O
deep O
session O
recommendation O
models O
. O

Customer O
Sessions O
A O
customer O
session O
is O
the O
search O
log O
before O
a O
final O
purchase O
action. O
It O
consists O
of O
customer-queryproduct O
interactions O
: O
a O
customer O
submits O
search O
queries O
obtains O
a O
list O
of O
products. O
The O
customer O
may O
take O
specific O
actions O
, O
including O
view O
and O
purchase O
on O
the O
retrieved O
products. O
Hence O
, O
a O
session O
contains O
two O
types O
of O
items O
: O
queries O
and O
products O
, O
and O
various O
relations O
between O
them O
established O
by O
customer O
actions O
. O

We O
define O
each O
session O
as O
a O
relational O
graph O
G O
= O
( O
V O
, O
E O
) O
that O
contains O
all O
queries O
and O
products O
in O
a O
session O
and O
their O
relations. O
The O
vertex O
set O
V O
= O
( O
Q O
, O
P O
) O
is O
partitioned O
into O
ordered B-HyperparameterName
query I-HyperparameterName
set I-HyperparameterName
Q I-HyperparameterName
and O
unordered B-HyperparameterName
product I-HyperparameterName
set I-HyperparameterName
P. I-HyperparameterName
The O
queries O
Q B-HyperparameterName
= O
( O
q O
1 O
, O
. O
. O
. O
, O
q O
n O
) O
are O
indexed O
by O
order O
of O
the O
customer O
's O
searches. O
The O
edge B-HyperparameterName
set I-HyperparameterName
E I-HyperparameterName
contains O
two O
types O
of O
edges O
: O
{ O
( O
q O
i O
, O
q O
j O
) O
, O
i O
< O
j O
} O
are O
one-directional O
edges O
that O
connect O
each O
query O
to O
its O
previous O
queries O
; O
and O
{ O
q O
i O
, O
p O
j O
, O
a O
ij O
} O
are O
bidirectional O
edges O
that O
connects O
the O
ith O
query O
and O
jth O
product O
, O
if O
the O
customer O
took O
action O
a O
ij O
on O
product O
p O
j O
retrieved O
by O
query O
q O
j O
. O

The O
queries O
and O
products O
are O
represented O
by O
textual O
descriptions. O
Specifically O
, O
each O
query O
is O
represented O
by O
customer-generated O
search O
keywords. O
Each O
product O
is O
represented O
with O
a O
table O
of O
textual O
attributes. O
Each O
product O
is O
guaranteed O
to O
have O
a O
product O
title O
and O
description. O
In O
this O
paper O
, O
we O
call O
" O
product O
sequence O
" O
as O
the O
concatenation O
of O
title O
and O
description. O
A O
product O
may O
have O
additional O
attributes O
, O
such O
as O
product O
type O
, O
color O
, O
brand O
, O
and O
manufacturer O
, O
depending O
on O
their O
specific O
categories O
. O

Our O
Method O
In O
this O
section O
we O
present O
the O
details O
of O
CERES. B-MethodName
We O
first O
describe O
our O
designed O
session O
pretraining O
task O
in O
Section O
3.1 O
, O
and O
then O
describe O
the O
model O
architecture O
of O
CERES B-MethodName
in O
Section O
3.2 O
. O

Graph-Conditioned O
Masked O
Language O
Modeling O
Task O

Suppose O
G O
= O
( O
V O
, O
E O
) O
is O
a O
graph O
on O
T O
text O
items O
as O
vertices O
, O
v O
1 O
, O
. O
. O
. O
, O
v O
T O
, O
each O
of O
which O
is O
a O
sequence O
of O
text O
tokens O
: O

v O
i O
= O
[ O
v O
i1 O
, O
. O
. O
. O
, O
v O
iT O
i O
] O
, O
i O
= O
1 O
, O
. O
. O
. O
, O
T O
. O

We O
propose O
graph-conditioned B-MethodName
masked I-MethodName
language I-MethodName
modeling I-MethodName
( O
GMLM B-MethodName
) O
, O
where O
masked O
tokens O
are O
predicted O
with O
both O
intra-item O
context O
and O
inter-item O
context O
: O

p O
GMLM B-MethodName
( O
v O
masked O
) O
= O
jth O
masked O
P O
( O
vij|G O
, O
{ O
v O
ik O
} O
kth O
unmasked O
) O
, O
( O
1 O
) O

which O
encourages O
the O
model O
to O
leverage O
information O
graph-level O
inter-item O
semantics O
efficiently O
in O
order O
to O
predict O
masked O
tokens. O
To O
optimize O
( O
1 O
) O
, O
we O
need O
to O
learn O
token-level O
embeddings O
that O
are O
infused O
with O
session-level O
information O
, O
which O
we O
introduce O
in O
Section O
3.2.2. O
Suppose O
certain O
tokens O
in O
the O
input O
sequence O
of O
items O
as O
masked O
( O
detailed O
below O
) O
, O
we O
optimize O
the O
predictions O
of O
the O
masked O
tokens O
with O
cross O
entropy O
loss. O
The O
pretraining O
framework O
is O
illustrated O
in O
Figure O
3. O
Token O
Masking O
Strategy O
. O

To O
mask O
tokens O
in O
long O
sequences O
, O
including O
product O
titles O
and O
descriptions O
, O
we O
follow O
( O
Devlin O
et O
al. O
, O
2018 O
) O
and O
choose O
15 O
% O
of O
the O
tokens O
for O
masking. O
For O
short O
sequences O
, O
including O
queries O
and O
product O
attributes O
, O
there O
is O
a O
50 O
% O
probability O
that O
a O
short O
sequence O
will O
be O
masked O
, O
and O
for O
those O
sequences O
50 O
% O
of O
their O
tokens O
are O
randomly O
selected O
for O
masking O
. O

Model O
Architecture O
To O
model O
the O
probability O
in O
( O
1 O
) O
, O
we O
design O
two O
key O
components O
in O
the O
CERES B-MethodName
model O
: O
1 O
) O
a O
Transformer-based O
item O
encoder O
, O
which O
produces O
token-level O
intra-item O
embeddings O
that O
contain O
context O
information O
within O
a O
single O
item O
; O
and O
2 O
) O
a O
graph-conditioned O
Transformer O
for O
session O
encoding O
, O
which O
produces O
session-level O
embeddings O
that O
encodes O
inter-item O
relations O
, O
and O
propagates O
the O
session O
information O
back O
to O
the O
token-level. O
We O
illustrate O
our O
model O
architecture O
in O
Figure O
2 O
. O

Item O
Transformer O
Encoder O
The O
session O
item O
encoder O
aims O
to O
encode O
intra-item O
textual O
information O
for O
each O
item O
in O
a O
session. O
We O
design O
the O
item O
encoder O
based O
on O
Transformers O
, O
which O
allows O
CERES B-MethodName
to O
leverage O
the O
expressive O
power O
of O
the O
self-attention O
mechanism O
for O
modeling O
domain-specific O
language O
in O
e-commerce O
sessions. O
Given O
an O
item O
i O
, O
the O
transformer-based O
item O
encoder O
compute O
its O
token O
embeddings O
as O
follows O
: O

[ O
vi1 O
, O
. O
. O
. O
, O
viT O
i O
] O
= O
Transformeritem O
( O
[ O
vi1 O
, O
. O
. O
. O
, O
viT O
i O
] O
) O
vi O
= O
Pool O
( O
[ O
vi1 O
, O
. O
. O
. O
, O
viT O
i O
] O
) O
, O
( O
2 O
) O

where O
v O
ij O
is O
the O
embedding O
of O
the O
jth O
token O
in O
the O
ith O
item O
, O
and O
v O
i O
is O
the O
pooled O
embedding O
of O
the O
ith O
item. O
At O
this O
stage O
, O
{ O
v O
ij O
} O
, O
{ O
v O
i O
} O
are O
embeddings O
that O
only O
encode O
the O
intra-item O
information. O
Details O
of O
Item O
Encoding. O
We O
detail O
the O
encoding O
method O
for O
the O
two O
types O
of O
items O
, O
queries O
and O
products O
, O
in O
the O
following O
paragraphs. O
Each O
query O
q O
i O
= O
[ O
q O
i1 O
, O
. O
. O
. O
, O
q O
iT O
i O
] O
is O
a O
sequence O
of O
tokens O
generated O
by O
customers O
as O
search O
keywords. O
We O
add O
a O
special O
token O
at O
the O
beginning O
of O
the O
queries O
, O
[ O
SEARCH O
] O
, O
to O
indicate O
that O
the O
sequence O
represents O
a O
customer O
's O
search O
keywords. O
Then O
, O
to O
obtain O
the O
token-level O
embedding O
of O
the O
queries O
and O
the O
pooled O
query O
embedding O
by O
taking O
the O
embedding O
of O
the O
special O
token O
[ O
SEARCH O
] O
. O

Each O
product O
p O
i O
is O
a O
table O
of O
K O
attributes O
: O
p O
1 O
, O
. O
. O
. O
, O
p O
K O
, O
where O
p O
1 O
is O
always O
the O
product O
sequence O
, O
which O
is O
the O
concatenation O
of O
product O
title O
and O
bullet O
description. O
Each O
attribute O

p O
k O
i O
= O
[ O
p O
k O
i1 O
, O
p O
k O
i2 O
, O
. O

. O
. O
] O
starts O
with O
a O
special O
token O
[ O
ATTRTYPE O
] O
, O
where O
ATTRTYPE O
is O
replaced O
with O
the O
language O
descriptor O
of O
the O
attribtue. O
Then O
, O
the O
Transformer O
is O
used O
to O
compute O
token O
and O
sentence O
embeddings O
for O
all O
attributes. O
The O
product O
embedding O
is O
obtained O
by O
average O
pooling O
of O
all O
attribute O
's O
sentence O
embeddings O
. O

Graph-Conditioned B-MethodName
Session I-MethodName
Transformer I-MethodName
The O
Graph-Conditioned B-MethodName
Session I-MethodName
Transformer I-MethodName
aims O
to O
infuse O
intra-item O
and O
inter-item O
information O
to O
produce O
item O
and O
token O
embeddings. O
For O
this O
purpose O
, O
we O
first O
design O
a O
position-aware B-MethodName
graph I-MethodName
neural I-MethodName
network I-MethodName
( O
PGNN B-MethodName
) O
to O
capture O
the O
interitem O
dependencies O
in O
a O
session O
graph O
to O
produce O

Item B-MethodName
Token I-MethodName
Embeddings I-MethodName
Latent B-MethodName
Conditioning I-MethodName
Tokens I-MethodName
Figure O
4 O
: O
Illustration O
of O
cross-attention O
over O
latent O
conditioning O
tokens. O
The O
item O
token O
embeddings O
perform O
self-attention O
as O
well O
as O
cross-attention O
over O
latent O
conditioning O
tokens O
, O
thus O
incorporating O
session-level O
information. O
Latent O
conditioning O
tokens O
perform O
selfattention O
to O
update O
their O
embeddings O
, O
but O
do O
not O
attend O
to O
item O
tokens O
to O
preserve O
session-level O
information O
. O

item O
embeddings. O
The O
effect O
of O
PGNN B-MethodName
is O
analyzed O
in O
Section O
4.4. O
Then O
conditioned O
on O
the O
PGNN-learned B-MethodName
item O
embedding O
, O
we O
propose O
a O
cross-attention O
Transformer O
, O
which O
produces O
infused O
item O
and O
token O
embeddings O
for O
the O
Graph-Conditioned B-MethodName
Masked I-MethodName
Language I-MethodName
Modeling I-MethodName
task I-MethodName
. O

Position-Aware B-MethodName
Graph I-MethodName
Neural I-MethodName
Network. I-MethodName
We O
use O
a O
GNN B-MethodName
to O
capture O
inter-item O
relations. O
This O
will O
allow O
CERES B-MethodName
to O
obtain O
item O
embeddings O
that O
encode O
the O
information O
from O
other O
locally O
correlated O
items O
in O
the O
session. O
Let O
[ O
v O
1 O
, O
. O
. O
. O
, O
v O
N O
] O
denote O
the O
item O
embeddings O
produced O
by O
the O
intra-item O
transformer O
encoder. O
We O
treat O
them O
as O
hidden O
states O
of O
nodes O
in O
the O
session O
graph O
G O
and O
feed O
them O
to O
the O
GNN B-MethodName
model O
, O
obtaining O
session-level O
item O
embeddings O

[ O
v O
h O
1 O
, O
. O
. O
. O
, O
v O
h O
N O
] O
. O

The O
items O
in O
a O
session O
graph O
are O
sequential O
according O
to O
the O
order O
the O
customers O
generated O
them O
. O

To O
let O
the O
GNN B-MethodName
model O
learn O
of O
the O
positional O
information O
of O
items O
, O
we O
train O
an O
item O
positional O
embedding O
in O
the O
same O
way O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2018 O
) O
trains O
positional O
embeddings O
of O
tokens. O
Before O
feeding O
the O
item O
embeddings O
to O
GNN B-MethodName
, O
the O
pooled O
item O
embeddings O
are O
added O
item O
positional O
embeddings O
according O
to O
their O
positions O
in O
the O
session O
's O
item O
sequence. O
In O
this O
way O
, O
the O
item O
embeddings O
{ O
v O
i O
} O
i∈V O
are O
encoded O
their O
positional O
information O
as O
well O
. O

Cross-Attention B-MethodName
Transformer. I-MethodName
Conditioned O
on O
PGNN B-MethodName
, O
we O
design O
a O
cross-attention O
transformer O
which O
propagates O
session-level O
information O
in O
PGNN-produced B-MethodName
item O
embeddings O
to O
all O
tokens O
to O
produce O
token O
embeddings O
that O
are O
infused O
with O
both O
intra-item O
and O
inter-item O
information O
. O

In O
order O
to O
propagate O
item O
embeddings O
to O
tokens O
, O
we O
treat O
item O
embeddings O
as O
latent O
tokens O
that O
can O
be O
treated O
as O
a O
" O
part O
" O
of O
item O
texts. O
for O
each O
item O
i O
, O
we O
first O
expand O
v O
h O
i O
to O
K O
latent O
conditioning O
tokens O
by O
using O
a O
multilayer O
perceptron O
module O
to O
map O
v O
h O
i O
to O
K O
embedding O
vectors O
[ O
v O
h O
i1 O
, O
. O
. O
. O
, O
v O
h O
iK O
] O
of O
the O
same O
size. O
For O
each O
item O
i O
, O
we O
compute O
its O
latent O
conditioning O
tokens O
by O
averaging O
all O
latent O
tokens O
in O
its O
neighborhood. O
Suppose O
N O
( O
i O
) O
is O
the O
set O
of O
all O
neighboring O
items O
in O
the O
session O
graph O
, O
itself O
included. O
In O
each O
position O
, O
we O
take O
the O
average O
of O
the O
latent O
token O
embeddings O
in O
N O
( O
i O
) O
as O
the O
kth O
latent O
conditioning O
token O
, O
v O
h O
ik O
, O
for O
the O
ith O
item. O
Then O
, O
we O
concatenate O
the O
latent O
conditioning O
token O
embeddings O
and O
the O
item O
token O
embeddings O
obtained O
by O
the O
session O
item O
encoder O
: O

[ O
v O
h O
i1 O
, O
. O
. O
. O
, O
v O
h O
iK O
, O
v O
i1 O
, O
. O
. O
. O
, O
v O
iN O
i O
] O
. O
( O
3 O
) O

Finally O
, O
we O
compute O
the O
token-level O
embeddings O
with O
session O
information O
by O
feeding O
the O
concatenated O
sequence O
to O
a O
shallow O
cross-attention O
Transformer. O
The O
cross-attention O
Transformer O
is O
of O
the O
same O
structure O
as O
normal O
Transformers. O
The O
difference O
is O
that O
we O
prohibit O
the O
latent O
conditioning O
tokens O
from O
attending O
over O
original O
item O
tokens O
to O
prevent O
the O
influx O
of O
intra-item O
information O
potentially O
diluating O
session-level O
information O
stored O
in O
latent O
conditioning O
tokens. O
Illustration O
of O
crossattention O
Transformer O
is O
provided O
in O
Figrue O
4 O
. O

We O
use O
the O
embeddings O
produced O
by O
this O
crossattention O
Transformer O
as O
the O
final O
embeddings O
for O
modeling O
the O
token O
probabilities O
in O
Equation O
( O
1 O
) O
and O
learning O
the O
masked O
language O
modeling O
tasks O
. O

During O
training O
, O
the O
model O
is O
encouraged O
to O
learn O
good O
token O
embeddings O
with O
the O
Item O
Transformer O
Encoder O
, O
as O
better O
embeddings O
{ O
v O
ij O
} O
N O
i O
j=1 O
is O
necessary O
to O
improve O
the O
quality O
of O
{ O
v O
c O
ij O
} O
N O
i O
j=1 O
. O
The O
Graph-Conditioned B-MethodName
Transformer I-MethodName
will O
be O
encouraged O
to O
produce O
high-quality O
session-level O
embeddings O
for O
the O
GMLM B-MethodName
task. O
Hence O
, O
CERES B-MethodName
is O
encouraged O
to O
produce O
high-quality O
embeddings O
that O
unify O
both O
intra-item O
and O
inter-item O
information O
. O

Finetuning O
When O
finetuning O
CERES B-MethodName
for O
downstream O
tasks O
, O
we O
first O
obtain O
session-level O
item O
embeddings. O
The O
session O
embedding O
is O
computed O
as O
the O
average O
of O
all O
item O
embeddings. O
To O
obtain O
embedding O
for O
a O
single O
item O
without O
session O
context O
, O
such O
as O
for O
retrieved O
items O
in O
recommendation O
tasks O
, O
only O
the O
Item O
Transformer O
Encoder O
is O
used O
. O

To O
measure O
the O
relevance O
of O
an O
item O
to O
a O
given O
session O
, O
we O
first O
transform O
the O
obtained O
embeddings O
by O
separate O
linear O
maps. O
Denote O
the O
transformed O
session O
embeddings O
as O
s O
and O
item O
embeddings O
as O
y. O
The O
similarity O
between O
them O
is O
computed O
by O
cosine O
similarity O
d O
cos O
( O
s O
, O
y O
) O
. O
To O
finetune O
the O
model O
, O
we O
optimize O
a O
hinge O
loss O
on O
the O
cosine O
similarity O
between O
sessions O
and O
items O
. O

Experiments O

Experiment O
Setup O
Dataset. O
We O
collected O
customer B-DatasetName
sessions I-DatasetName
from O
Amazon O
for O
pretraining O
and O
finetuning O
on O
downstream O
tasks. O
468,199,822 O
customer B-DatasetName
sessions I-DatasetName
are O
collected O
from O
August O
1 O
2020 O
to O
August O
31 O
2020 O
for O
pretraining. O
30,000 O
sessions O
are O
collected O
from O
September O
2020 O
to O
September O
7 O
2020 O
for O
downstream O
tasks. O
The O
pretraining O
and O
downstreaming O
datasets O
are O
from O
disjoint O
time O
spans O
to O
prevent O
data O
leakage. O
All O
data O
are O
cleaned O
and O
anonymized O
so O
that O
no O
personal O
information O
about O
customers O
was O
used. O
Each O
session O
is O
collected O
as O
follows O
: O
when O
a O
customer O
perform O
a O
purchase O
action O
, O
we O
backtrace O
all O
actions O
by O
the O
customer O
in O
600 O
seconds O
before O
the O
purchase O
until O
a O
previous O
purchase O
is O
encountered. O
The O
actions O
of O
customers O
include O
: O
1 O
) O
search O
, O
2 O
) O
view O
, O
3 O
) O
, O
add-to-cart O
, O
and O
4 O
) O
purchase. O
Search O
action O
is O
associated O
with O
customer O
generated O
query O
keywords. O
View O
, O
add-to-cart O
, O
and O
purchase O
are O
associated O
with O
the O
target O
products. O
All O
the O
products O
in O
the O
these O
sessions O
are O
gathered O
with O
their O
product O
title O
, O
bullet O
description O
, O
and O
various O
other O
attributes O
, O
including O
color O
, O
manufacturer O
, O
product O
type O
, O
size O
, O
etc. O
In O
total O
, O
we O
have O
37,580,637 O
products. O
The O
sessions O
have O
an O
average O
of O
3.24 O
queries O
and O
4.36 O
products. O
Queries O
have O
on O
average O
5.63 O
tokens O
, O
while O
product O
titles O
and O
bullet O
descriptions O
have O
averagely O
17.42 O
and O
96.01 O
tokens O
. O

Evaluation O
Tasks O
and O
Metrics. O
We O
evaluate O
all O
the O
compared O
models O
on O
the O
following O
tasks O
: O
1 O
) O
Product B-TaskName
Search. I-TaskName
In O
this O
task O
, O
given O
observed O
customer O
behaviors O
in O
a O
session O
, O
the O
model O
is O
asked O
to O
predict O
which O
product O
will O
be O
purchased O
from O
a O
pool O
of O
candidate O
products. O
The O
purchased O
products O
are O
removed O
from O
sessions O
to O
avoid O
trivial O
inference. O
The O
candidate O
product O
pool O
is O
the O
union O
of O
all O
purchased O
products O
in O
the O
test O
set O
and O
the O
first O
10 O
products O
returned O
by O
the O
search O
engine O
of O
all O
sessions O
in O
the O
test O
set O
. O

2 O
) O
Query B-TaskName
Search. I-TaskName
Query O
Search O
is O
a O
recommendation O
task O
where O
the O
model O
retrieves O
next O
queries O
for O
customers O
which O
will O
lead O
to O
a O
purchase. O
Given O
a O
session O
, O
we O
hide O
the O
last O
query O
along O
with O
products O
associated O
with O
it O
, O
i.e. O
viewed O
or O
purchased O
with O
the O
removed O
query. O
Then O
, O
we O
ask O
the O
model O
to O
predict O
the O
last O
query O
from O
a O
pool O
of O
candidate O
queries. O
The O
candidate O
query O
pool O
consists O
of O
all O
last O
queries O
in O
the O
test O
set O
. O

3 O
) O
Entity B-TaskName
Linking. I-TaskName
In O
this O
task O
we O
try O
to O
understand O
the O
deeper O
semantics O
of O
customer O
sessions. O
Specifically O
, O
if O
customer O
purchases O
a O
product O
in O
a O
session O
, O
the O
task O
is O
to O
predict O
the O
attributes O
of O
the O
purchased O
product O
from O
the O
rest O
contexts O
in O
the O
session. O
In O
total O
, O
we O
have O
60K O
possible O
product O
attributes O
. O

Baselines. O
The O
compared O
baselines O
can O
be O
categorized O
into O
three O
groups O
: O

1 O
) O
General-domain O
pretrained O
language O
models O
which O
include O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2018 O
) O
, O
RoBERTa B-MethodName
, O
and O
ELECTRA B-MethodName
( O
Clark O
et O
al. O
, O
2020 O
) O
. O
These O
models O
are O
state-of-the-art O
pretrained O
language O
models O
, O
which O
can O
serve O
as O
general-purpose O
language O
encoders O
for O
items O
and O
enable O
downstream O
session-related O
tasks. O
Specifically O
, O
the O
language O
encoders O
produce O
item O
embeddings O
first O
, O
and O
compose O
session O
embeddings O
by O
pooling O
the O
items O
in O
sessions. O
To O
retrieve O
items O
for O
sessions O
, O
one O
can O
compare O
the O
cosine O
similarity O
between O
sessions O
and O
retrieved O
items O
. O

2 O
) O
Pretrained O
session O
models O
which O
are O
pretrained O
models O
on O
e-commerce O
session O
data. O
Specifically O
, O
we O
pretrain O
the O
following O
language O
models O
using O
our O
session O
data O
: O
a O
) O
Product-BERT B-MethodName
, O
which O
is O
a O
domain-specific O
BERT B-MethodName
model O
pretrained O
with O
product O
information O
; O
b O
) O
SQSP-BERT B-MethodName
, O
where O
SQSP B-MethodName
is O
short O
for O
Single-query O
Single-Product. O
SQSP-BERT B-MethodName
is O
pretrained O
on O
query-product O
interaction O
pairs O
with O
language O
modeling O
and O
contrastive O
learning O
objectives. O
They O
are O
used O
in O
the O
same O
manner O
in O
downstream O
tasks O
as O
general-domain O
pretrained O
language O
models. O
The O
detailed O
configurations O
are O
provided O
in O
the O
Appendix O
. O

3 O
) O
Session-based O
recommendation O
methods O
including O
SR-GNN B-MethodName
( O
Wu O
et O
al. O
, O
2019b O
) O
and O
NISER+ B-MethodName
( O
Gupta O
et O
al. O
, O
2019 O
) O
, O
which O
are O
state-ofthe-art O
models O
for O
session-based O
product O
recommendation O
on O
traditional O
benchmarks O
, O
including O
YOOCHOOSE B-DatasetName
and O
DIGINETICA B-DatasetName
; O
and O
Nvidia B-MethodName
's I-MethodName
MERLIN I-MethodName
( O
Mobasher O
et O
al. O
, O
2001 O
) O
, O
which O
is O
the O
bestperforming O
model O
in O
the O
recent O
SIGIR O
Next O
Items O
Prediction O
challenge O
( O
Kallumadi O
et O
al. O
, O
2021 O
) O
To O
evaluate O
the O
performance O
on O
these O
tasks O
, O
we O
employ O
standard O
metrics O
for O
recommendation O
systems O
, O
including O
MAP B-MetricName
@ I-MetricName
K I-MetricName
, I-MetricName
and O
Recall B-MetricName
@ I-MetricName
K I-MetricName
. O

Implementation O
Details O
The O
implementation O
details O
for O
pretraining O
and O
finetuning O
stages O
are O
described O
as O
follows O
. O

Pretraining O
details. O
We O
developed O
our O
model O
based O
on O
Megatron-LM B-MethodName
( O
Shoeybi O
et O
al. O
, O
2019 O
) O
. O
We O
used O
768 B-HyperparameterValue
as O
the O
hidden B-HyperparameterName
size I-HyperparameterName
, O
a O
12-layer O
transformer O
blocks O
as O
the O
backbone O
language O
model O
, O
a O
twolayer O
Graph B-HyperparameterName
Attention I-HyperparameterName
Network I-HyperparameterName
and O
three-layer O
Transformer O
as O
the O
conditioned O
language O
model O
layers. O
In O
total O
, O
our O
model O
has O
141M O
parameters. O
The O
model O
is O
trained O
for O
300,000 O
steps O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
512 B-HyperparameterValue
sessions. O
The O
parameters O
are O
updated O
with O
Adam O
, O
with O
peak O
learning B-HyperparameterName
rate I-HyperparameterName
as O
3e B-HyperparameterValue
− I-HyperparameterValue
5 I-HyperparameterValue
, O
1 B-HyperparameterValue
% I-HyperparameterValue
steps B-HyperparameterName
for O
linear O
warm-up O
, O
and O
linear O
learning B-HyperparameterName
rate I-HyperparameterName
decay O
after O
warm-up O
until O
the O
learning B-HyperparameterName
rate I-HyperparameterName
reaches O
the O
minimum O
1e B-HyperparameterValue
− I-HyperparameterValue
5. I-HyperparameterValue
We O
trained O
our O
model O
on O
16 O
A400 O
GPUs O
on O
Amazon O
AWS O
for O
one O
week. O
Finetuning O
details. O
For O
each O
downstream O
task O
, O
we O
collected O
30,000 O
sessions O
for O
training O
, O
3000 O
for O
validation O
and O
5000 O
for O
testing. O
For O
each O
of O
the O
pretrained O
model O
, O
we O
finetune O
them O
for O
10 B-HyperparameterValue
epochs B-HyperparameterName
with O
a O
maximal O
learning B-HyperparameterName
rate I-HyperparameterName
chosen O
from O
[ O
1e-4 O
, O
1e-5 O
, O
5e-5 O
, O
5e-6 O
] O
to O
maximize O
MAP B-MetricName
@ I-MetricName
1 I-MetricName
on O
the O
validation O
set. O
The O
rest O
of O
the O
configuration O
of O
optimizers O
is O
the O
same O
as O
in O
pretraining O
. O

Main O
Results O

Product B-TaskName
Search I-TaskName
Table O
1 O
shows O
the O
performance O
of O
different O
methods O
for O
the O
product B-TaskName
search I-TaskName
task. O
We O
observe O
that O
CERES B-MethodName
outperforms O
domain-specific O
methods O
by O
more O
than O
1 B-MetricValue
% I-MetricValue
and O
general-domain O
methods O
by O
over O
6 B-MetricValue
% I-MetricValue
in O
MAP B-MetricName
@ I-MetricName
1. O
The O
second O
best O
performing O
model O
is O
Product-BERT B-MethodName
, O
which O
is O
pretrained O
on O
product O
information O
alone O
. O

We O
also O
compared O
with O
session-based O
recommendation O
systems. O
SR-GNN B-MethodName
and O
NISER+ B-MethodName
model O
only O
session O
graph O
structure O
but O
not O
text O
semantics O
; O
hence O
they O
have O
limited O
performance O
because O
of O
the O
suboptimal O
representation O
of O
session O
items. O
While O
MERLIN B-MethodName
can O
capture O
better O
text O
semantics O
, O
its O
text O
encoder O
is O
not O
trained O
on O
domain-specific O
e-commerce O
data. O
While O
it O
can O
outperform O
generaldomain O
methods O
, O
its O
performance O
is O
lower O
than O
Product-BERT B-MethodName
and O
CERES. B-MethodName
The O
benefits O
of O
joint O
modeling O
of O
text O
and O
graph O
data O
and O
the O
Graph-Conditioned B-MethodName
MLM I-MethodName
allow O
CERES B-MethodName
to O
outperform O
existing O
session O
recommendation O
models O
. O

Query B-TaskName
Search I-TaskName
Table O
2 O
shows O
the O
performance O
of O
different O
methods O
on O
Query B-TaskName
Search. I-TaskName
Query B-TaskName
Search I-TaskName
is O
a O
more O
difficult O
task O
than O
Product B-TaskName
Search I-TaskName
because O
customergenerated O
next O
queries O
are O
of O
higher O
variance. B-MetricName
In O
this O
challenging O
task O
, O
CERES B-MethodName
outperforms O
the O
best O
domain-specific O
model O
by O
over O
7 B-MetricValue
% I-MetricValue
and O
generaldomain O
model O
by O
12 B-MetricValue
% I-MetricValue
in O
all O
metrics O
. O

Entity B-TaskName
Linking I-TaskName
Table O
3 O
shows O
the O
results O
on O
Entity B-TaskName
Linking. I-TaskName
Similar O
to O
Query B-DatasetName
Search I-DatasetName
, O
this O
task O
also O
requires O
the O
models O
to O
tie O
text O
semantics O
( O
queries O
/ O
product O
attributes O
) O
to O
a O
customer B-DatasetName
session I-DatasetName
, O
which O
requires O
a O
deeper O
understanding O
of O
customer O
preferences. O
It O
is O
easier O
than O
Query B-TaskName
Search I-TaskName
as O
product O
attributes O
are O
of O
lower O
variance. O
However O
, O
the O
product O
attributes O
that O
the O
customer O
prefer O
rely O
more O
on O
session O
information O
, O
as O
they O
may O
have O
been O
reflected O
in O
the O
past O
search O
queries O
and O
viewed O
products. O
In O
this O
task O
, O
CERES B-MethodName
outperforms O
domain-specific O
models O
and O
general-domain O
models O
by O
averagely O
9 B-MetricValue
% I-MetricValue
in O
MAP B-MetricName
@ I-MetricName
1 I-MetricName
and O
6 B-MetricValue
% I-MetricValue
in O
MAP B-MetricName
@ I-MetricName
32 I-MetricName
and O
MAP B-MetricName
@ I-MetricName
64 I-MetricName
. O

Further O
Analysis O
and O
Ablation O
Studies O
In O
this O
section O
we O
present O
further O
studies O
to O
understand O
: O
1 O
) O
the O
effect O
of O
training O
data O
sizes O
in O
the O
downstream O
task O
; O
2 O
) O
the O
effects O
of O
different O
components O
in O
CERES B-MethodName
for O
both O
the O
pretraining O
and O
finetuning O
stages. O
following O
observations O
: O

CERES B-MethodName
is O
highly O
effective O
when O
training O
data O
are O
scarce O
. O

We O
compare O
CERES B-MethodName
with O
two O
strongest O
baselines O
( O
BERT B-MethodName
, O
and O
Product-BERT B-MethodName
) O
when O
the O
training O
sample O
size O
varies. O
Figure O
5 O
shows O
the O
MAP B-MetricName
@ I-MetricName
64 I-MetricName
scores O
of O
these O
methods O
on O
Product B-TaskName
Search I-TaskName
and O
Query B-TaskName
Search I-TaskName
when O
training O
size O
varies. O
Clearly O
, O
the O
advantage O
of O
CERES B-MethodName
is O
greater O
when O
training O
data O
is O
extremely O
small. O
With O
a O
training O
size O
of O
300 O
, O
CERES B-MethodName
can O
achieve O
a O
decent O
performance O
of O
about O
37.55 B-MetricValue
% I-MetricValue
in O
Product B-TaskName
Search I-TaskName
and O
36.37 B-MetricValue
% O
in O
Query B-TaskName
Search I-TaskName
, O
while O
the O
baseline O
models O
can O
not O
be O
trained O
sufficiently O
with O
such O
small-sized O
data. O
This O
shows O
that O
the O
efficient O
utilization O
of O
session-level O
information O
in O
pretraining O
and O
fine-tuning O
stages O
make O
the O
model O
more O
data O
efficient O
than O
other O
pretrained O
models O
. O

Graph-Conditioned B-MethodName
Transformer I-MethodName
is O
Vital O
to O
Pretraining. O
Without O
the O
Graph-Conditioned B-MethodName
Transformer I-MethodName
in O
pretraining O
, O
our O
model O
is O
essentially O
the O
same O
as O
domain-specific O
baselines O
, O
such O
as O
Product-BERT B-MethodName
, O
which O
are O
trained O
on O
session O
data O
but O
only O
with O
intra-item O
text O
signals. O
While O
SQSP-BERT B-MethodName
has O
access O
to O
session-level O
information O
when O
maximizing O
the O
masked O
language O
modeling O
objective O
, O
the O
lack O
of O
a O
dedicated O
module O
for O
GMLM B-MethodName
results O
in O
worse O
performance O
, O
as O
shown O
in O
the O
main O
experiment O
results. O
We O
could O
train O
the O
Graph-Conditioned B-MethodName
Transformer I-MethodName
from O
scratch O
in O
the O
finetuning O
stage. O
We O
present O
a O
model O
called O
CERES B-MethodName
w O
/ O
o O
Pretrain O
, O
which O
attaches O
the O
Graph-Conditioned B-MethodName
Session I-MethodName
Transformer I-MethodName
to O
Product-BERT B-MethodName
as O
the O
Item O
Transformer O
Encoder. O
As O
shown O
in O
Figure O
6 O
, O
this O
ablation O
method O
achieves O
MAP B-MetricName
@ I-MetricName
64 I-MetricName
scores O
of O
89.341 B-MetricValue
% I-MetricValue
in O
Product B-TaskName
Search I-TaskName
, O
64.890 B-MetricValue
% I-MetricValue
in O
Query B-TaskName
Search I-TaskName
, O
and O
74.031 B-MetricValue
% I-MetricValue
in O
Entity B-TaskName
Linking I-TaskName
, O
which O
are O
below O
Product-BERT. B-MethodName
This O
shows O
that O
the O
pretraining O
stage O
of O
the O
Graph-Conditioned B-MethodName
Transformer I-MethodName
is O
necessary O
to O
facilitate O
its O
ability O
to O
aggregate O
and O
propagate O
session-level O
information O
for O
downstream O
tasks O
. O

Graph-Conditioned B-MethodName
Transformer I-MethodName
Improves O
Item-level O
Embeddings. O
We O
also O
present O
CERES B-MethodName
w O
/ O
o O
Cond O
, O
which O
has O
the O
same O
pretrained O
model O
as O
CERES B-MethodName
, O
but O
only O
uses O
the O
Item O
Transformer O
Encoder O
in O
the O
finetuning O
stage. O
The O
Item O
Transformer O
Encoder O
is O
used O
to O
compute O
session O
item O
embeddings O
that O
contain O
only O
item-level O
information O
, O
and O
then O
takes O
the O
average O
of O
these O
embeddings O
as O
session O
embedding. O
As O
shown O
in O
Figure O
6 O
, O
CERES B-MethodName
w O
/ O
o O
Cond O
acheives O
94.741 B-MetricValue
% I-MetricValue
, O
72.175 B-MetricValue
% I-MetricValue
, O
and O
81.03 B-MetricValue
% I-MetricValue
respectively O
in O
Product B-TaskName
Search I-TaskName
, O
Query B-TaskName
Search I-TaskName
, O
and O
Entity B-TaskName
Linking I-TaskName
, O
observing O
a O
drop B-MetricName
of O
0.1 B-MetricValue
% I-MetricValue
to O
0.2 B-MetricValue
% I-MetricValue
in O
performance O
compared O
with O
CERES. B-MethodName
The O
performance O
drop O
is O
minor O
and O
CERES B-MethodName
w O
/ O
o O
Cond O
still O
outperforms O
baseline O
pretrained O
language O
models. O
Hence O
, O
the O
Graph-Conditioned B-MethodName
Transformer I-MethodName
in O
the O
pretraining O
stage O
helps O
the O
Item O
Transformer O
Encoder O
to O
learn O
better O
item-level O
embeddings O
that O
can O
be O
used O
for O
more O
effective O
leveraging O
of O
session O
information O
in O
the O
downstream O
tasks O
. O

Graph B-MethodName
Neural I-MethodName
Networks I-MethodName
Improve O
Representation O
of O
Sessions. O
In O
CERES B-MethodName
w O
/ O
o O
GNN B-MethodName
, O
we O
pretrain O
a O
CERES B-MethodName
model O
without O
a O
Graph B-MethodName
Neural I-MethodName
Network. I-MethodName
Specifically O
, O
CERES B-MethodName
w O
/ O
o O
GNN B-MethodName
skips O
the O
neighborhood O
information O
aggregation O
for O
items O
, O
and O
uses O
item-level O
embeddings O
obtained O
by O
the O
Item O
Transformer O
Encoder O
directly O
as O
latent O
conditioning O
tokens. O
We O
train O
and O
finetune O
this O
model O
with O
the O
same O
setup O
as O
CERES. B-MethodName
Without O
GNN B-MethodName
, O
the O
model O
's O
performance O
is O
consistently O
lower O
than O
CERES B-MethodName
, O
achieving O
93.453 B-MetricValue
% I-MetricValue
, O
71.231 B-MetricValue
% I-MetricValue
, O
80.26 B-MetricValue
% I-MetricValue
MAP B-MetricName
@ I-MetricName
64 I-MetricName
in O
three O
downstream O
tasks O
, O
observing O
a O
1.13 B-MetricValue
% I-MetricValue
performance O
drop. B-MetricName
This O
shows O
that O
GNN B-MethodName
's I-MethodName
aggregation O
of O
information O
can O
help O
item-level O
embeddings O
encode O
more O
session-level O
information O
, O
improving O
performance O
in O
downstream O
tasks O
. O

Model O
Efficiency. O
CERES B-MethodName
has O
additional O
few O
GNN B-MethodName
and O
Transformer O
layers O
attached O
to O
the O
end O
of O
the O
model. O
The O
additional O
layers O
bring O
∼20 O
% O
additional O
inference O
time O
compared O
to O
standard O
BERT B-MethodName
with O
12 B-HyperparameterValue
layers B-HyperparameterName
and O
768 B-HyperparameterValue
hidden B-HyperparameterName
size I-HyperparameterName
. O

Conclusion O
We O
proposed O
a O
pretraining O
framework O
, O
CERES B-MethodName
, O
for O
learning O
representations O
for O
semi-structured O
ecommerce O
sessions. O
We O
are O
the O
first O
to O
jointly O
model O
intra-item O
text O
and O
inter-item O
relations O
in O
session O
graphs O
with O
an O
end-to-end O
pretraining O
framework. O
By O
modeling O
Graph-Conditioned B-MethodName
Masked I-MethodName
Language I-MethodName
Modeling I-MethodName
, O
our O
model O
is O
encouraged O
to O
learn O
high-quality O
representations O
for O
both O
intraitem O
and O
inter-item O
information O
during O
its O
pretraining O
on O
massive O
unlabeled O
session O
graphs. O
Furthermore O
, O
as O
a O
generic O
session O
encoder O
, O
our O
model O
enabled O
effective O
leverage O
of O
session O
information O
in O
downstream O
tasks. O
We O
conducted O
extensive O
experiments O
and O
ablation O
studies O
on O
CERES B-MethodName
in O
comparison O
to O
state-of-the-art O
pretrained O
models O
and O
recommendation O
systems. O
Experiments O
show O
that O
CERES B-MethodName
can O
produce O
higher O
quality O
text O
representations O
as O
well O
as O
better O
leverage O
of O
session O
graph O
structure O
, O
which O
are O
important O
to O
many O
ecommerce O
related O
tasks O
, O
including O
product B-TaskName
search I-TaskName
, O
query B-TaskName
search I-TaskName
, O
and O
query B-TaskName
understanding I-TaskName
. O

A O
Details O
on O
Session O
Data O
A.1 O
Product O
Attributes O
. O

A O
product O
is O
represented O
with O
a O
table O
of O
attributes. O
Each O
product O
is O
guaranteed O
to O
have O
a O
product O
title O
and O
bullet O
description. O
In O
this O
paper O
, O
we O
regard O
the O
product O
title O
as O
the O
representative O
sequence O
of O
the O
product O
, O
called O
" O
product O
sequence O
" O
. O
A O
product O
may O
have O
additional O
attributes O
, O
such O
as O
product O
type O
, O
color O
, O
brand O
, O
and O
manufacturer O
, O
depending O
on O
specific O
products O
. O

A.2 O
Alternative O
Pretraining O
Corpora O
In O
this O
section O
we O
introduce O
alternative O
pretraining O
corpora O
that O
encode O
information O
in O
a O
session O
, O
including O
products O
and O
queries O
, O
but O
not O
treating O
sessions O
as O
a O
whole O
. O

A.2.1 O
Product B-DatasetName
Corpus I-DatasetName
In O
this O
corpus O
, O
we O
gathered O
all O
product O
information O
that O
appeared O
in O
the O
sessions O
from O
August O
2020 O
to O
September O
2020. O
Each O
product O
will O
have O
descriptions O
such O
as O
product O
title O
and O
bullet O
description O
, O
and O
other O
attributes O
like O
entity O
type O
, O
product O
type O
, O
manufacturer O
, O
etc. O
Particularly O
, O
bullet O
description O
is O
composed O
of O
several O
lines O
of O
descriptive O
facts O
about O
the O
product. O
All O
products O
without O
titles O
are O
removed. O
Each O
of O
the O
remaining O
product O
forms O
a O
paragraph O
, O
where O
the O
product O
title O
comes O
as O
the O
first O
sentence O
, O
followed O
by O
the O
entries O
of O
bullet O
descriptions O
each O
as O
a O
sentence O
, O
and O
product O
attributes. O
An O
example O
document O
in O
this O
corpora O
is O
as O
follows O
: O

[ O
Title O
] O
product O
title O
[ O
Description O
] O

A.2.3 O
Session B-DatasetName
Corpus I-DatasetName
In O
this O
corpus O
, O
we O
treat O
each O
session O
as O
a O
document O
and O
sequentially O
put O
text O
representations O
of O
items O
in O
a O
session O
to O
the O
document O
with O
special O
tokens O
indicating O
the O
fields O
of O
items. O
An O
example O
document O
looks O
like O
the O
follows O
: O

[ O
SEARCH O
] O
keywords O
1 O
[ O
SEARCH O
] O
keywords O
2 O
[ O
CLICK O
] O
[ O
TITLE O
] O
product O
1 O
[ O
SEARCH O
] O
keywords O
3 O
[ O
PURCHASE O
] O

[ O
TITLE O
] O
product O
2 O

In O
this O
example O
, O
the O
customer O
first O
attempted O
to O
search O
with O
keywords O
1 O
and O
then O
modified O
the O
keywords O
to O
keywords O
2. O
The O
customer O
then O
clicked O
on O
product O
1. O
At O
last O
, O
the O
customer O
modified O
his O
search O
to O
keywords O
3 O
and O
purchased O
product O
2. O
In O
this O
corpus O
, O
session O
information O
is O
present O
in O
a O
document O
, O
but O
the O
specific O
relations O
between O
elements O
are O
not O
specified. O
The O
comparison O
of O
different O
datasets O
are O
in O
Table O
5 O
. O

A.3 O
Alternative O
Pretraining O
Methods O
We O
introduce O
the O
alternative O
pretraining O
models O
. O

• O
Product-Bert. B-MethodName
It O
is O
pretrained O
on O
the O
Product B-DatasetName
Corpus. I-DatasetName
Specifically O
, O
we O
treat O
each O
product O
in O
the O
Product B-DatasetName
Corpus I-DatasetName
as O
an O
article. O
Product O
titles O
is O
always O
the O
first O
sentence O
, O
followed O
by O
paragraphs O
of O
bullet O
descriptions O
, O
which O
can O
contain O
multiple O
sentences. O
Then O
, O
each O
additional O
product O
attribute O
is O
a O
sentence O
added O
after O
the O
bullet O
descriptions. O
5 O
: O
Comparision O
of O
different O
pretraining O
dataset. O
Product B-DatasetName
Corpus I-DatasetName
has O
access O
only O
to O
product O
information. O
SQSP B-MethodName
models O
on O
the O
queries O
and O
query-product O
relations O
, O
without O
access O
to O
session O
context. O
Session B-DatasetName
Corpus I-DatasetName
has O
access O
to O
contextual O
information O
in O
a O
session O
, O
but O
does O
not O
model O
on O
relations O
between O
objects. O
Session-Graph O
has O
access O
to O
all O
information O
and O
models O
on O
the O
relational O
nature O
of O
nodes O
in O
the O
session O
graph O
. O

Product B-MethodName
Bert I-MethodName
is O
trained O
for O
300,000 B-HyperparameterValue
steps B-HyperparameterName
, O
with O
a O
12-layer B-HyperparameterValue
transformer O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
6144 B-HyperparameterValue
and O
peak B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
of O
1e-3 B-HyperparameterValue
, O
1 B-HyperparameterValue
% I-HyperparameterValue
linear B-HyperparameterName
warm-up I-HyperparameterName
steps I-HyperparameterName
, O
and O
1e−2 B-HyperparameterValue
linear B-HyperparameterName
weight I-HyperparameterName
decay I-HyperparameterName
to O
a O
minimum B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
of O
1e-5 B-HyperparameterValue
. O

• O
SQSP-Bert. B-MethodName
It O
is O
pretrained O
on O
SQSP B-MethodName
Corpus. O
The O
SQSP B-MethodName
Bert I-MethodName
uses O
the O
same O
Transformer O
backbone O
as O
Product B-MethodName
Bert. I-MethodName
Given O
each O
query-product O
pair O
, O
SQSP B-MethodName
feeds O
the O
text O
pair O
sequence O
to O
the O
Transformer O
for O
token O
embeddings O
for O
masked O
language O
modeling O
loss. O
In O
addition O
to O
language O
modeling O
, O
for O
each O
queryproduct O
pair O
, O
we O
sample O
a O
random O
product O
for O
the O
query O
as O
a O
negative O
query-product O
pair O
. O

The O
text O
pair O
sequence O
of O
the O
negative O
sample O
is O
also O
fed O
to O
the O
Transformer. O
Then O
, O
a O
discriminator O
is O
trained O
in O
the O
pretraining O
stage O
to O
distinguish O
the O
ground-truth O
query-product O
pairs O
and O
randomly O
sampled O
pairs. O
The O
discriminator O
's O
classification O
loss O
should O
serve O
as O
a O
contrastive O
loss O
. O

SQSP B-MethodName
Bert I-MethodName
is O
trained O
with O
the O
same O
configuration O
of O
Product B-MethodName
Bert I-MethodName
. O

B O
Details O
on O
Evaluation O
Metrics O
Mean B-MetricName
Average I-MetricName
Precision. I-MetricName
Suppose O
that O
for O
a O
session O
, O
m O
items O
are O
relevant O
and O
N O
items O
are O
retrieved O
by O
the O
model O
, O
the O
Average B-MetricName
Precision I-MetricName
( O
AP B-MetricName
) O
of O
a O
session O
is O
defined O
as O

AP B-MetricName
@ I-MetricName
N I-MetricName
= O
1 O
min O
( O
m O
, O
N O
) O
N O
k=1 O
P O
( O
k O
) O
rel O
( O
k O
) O
, O
( O
4 O
) O

where O
P B-MetricName
( I-MetricName
k I-MetricName
) I-MetricName
is O
the O
precision O
of O
the O
top O
k O
retrieved O
items O
, O
and O
rel O
( O
k O
) O
is O
an O
indicator O
function O
of O
whether O
the O
kth O
item O
is O
relevant. O
As O
we O
have O
at O
most O
one O
relevant O
item O
for O
each O
session O
, O
the O
above O
metric O
reduces O
to O
1 O
r O
, O
where O
r O
is O
the O
rank O
of O
the O
relevant O
item O
in O
the O
retrieved O
list O
, O
and O
k O
= O
∞ O
when O
the O
relevant O
item O
is O
not O
retrieved. O
MAP B-MetricName
@ I-MetricName
N I-MetricName
averages O
AP B-MetricName
@ I-MetricName
N I-MetricName
over O
all O
sessions O
, O

MAP B-MetricName
@ I-MetricName
N I-MetricName
= O
1 O
|S| O
s∈S O
1 O
r O
s O
( O
5 O
) O

where O
r O
s O
is O
the O
rank O
of O
the O
relevant O
item O
for O
a O
specific O
session O
s. O
MAP B-MetricName
in O
this O
case O
is O
equivalent O
to O
MRR B-MetricName
. O

Mean B-MetricName
Average I-MetricName
Precision I-MetricName
by I-MetricName
Queries I-MetricName
( O
MAPQ B-MetricName
) O
. O
Different O
from O
MAP B-MetricName
, O
MAPQ B-MetricName
averages O
AP B-MetricName
over O
last O
queries O
instead O
of O
sessions. O
Suppose O
Q O
is O
the O
set O
of O
unique O
last O
queries O
, O
and O
S O
( O
q O
) O
, O
q O
∈ O
Q O
is O
the O
set O
of O
sessions O
whose O
last O
queries O
are O
q O
, O
then O
the O
average B-MetricName
precision I-MetricName
for O
one O
query O
q O
is O

1 O
k O
i=1 O
rel O
( O
k O
) O
N O
k=1 O
min O
( O
1 O
, O
rs≤k O
rel O
( O
k O
) O
k O
) O
( O
6 O
) O

then O
we O
sum O
over O
all O
queries O
to O
obtain O
MAPQ B-MetricName
@ I-MetricName
N I-MetricName
. O

Mean B-MetricName
Reciprocal I-MetricName
Rank I-MetricName
by I-MetricName
Queries I-MetricName
( O
MRRQ B-MetricName
) O
. O
MRRQ B-MetricName
averages O
MRR B-MetricName
over O
session O
last O
queries O
instead O
of O
sessions O
. O

M B-MetricName
RRQ I-MetricName
@ I-MetricName
N I-MetricName
= O
1 O
|Q| O
q∈Q O
max O
s∈S O
( O
q O
) O
( O
r O
s O
) O
( O
7 O
) O

Recall. O
Recall B-MetricName
@ I-MetricName
N I-MetricName
calculates O
the O
percentage O
of O
sessions O
whose O
relevant O
items O
were O
retrieved O
among O
the O
top O
N O
predictions O
. O

